# Review_tones
Изучение подходов к решению задачи анализа тональности коротких текстов.
Задача анализа тональности текста, в упрощенном виде, может быть сведена к задаче бинарной 
классификации («положительное отношение»/«отрицательное отношение»). Решение данной 
задачи позволяет автоматизировать оценку удовлетворенности аудитории.

В качестве данных используем файл с отзывами doc_comment_summary.xlsx. с сайта linis-crowd.org. В данных присутствовуют две колонки: данные с текстом и значение тональности. 
Предполагается, что последняя колонка содержит только целые числа от -2 до 2, где -2 – крайне 
негативная тональность, 2 – крайне позитивная, 0 – нейтральная. Записи с пропусками, нейтральной тональностью и нечисловыми значениями удаляются.
Одним из самых простых методов извлечения числовых признаков из текстовых документов
является частотный метод. Можно учитывать как частоты слов, так и частоты букв, в общем случае 
отдельные элементы, на которые разбивается текст, имеются токенами. Важно отметить, что 
токенами могут быть не только буквы/символы или слова, но их последовательности из n-токенов, 
так называемые n-граммы. 

Существует два подхода к извлечению признаков:
1. Взять достаточно большой корпус текстов, построить словарь и насчитать статистики слов. 
Перед обучением модели пересчитать статистики для текущего набора данных.
2. Извлекать словарь и насчитать статистики непосредственно из обучающей выборки перед 
обучением модели.

Первый подход может дать лучшее качество, однако более трудоемкий и ресурсоемкий (чем 
больше словарь, тем больше оперативной памяти и тем мощнее процессор могут потребоваться). 
Во втором случае, как правило, имеем меньший словарь, и как следствие, более ограниченную 
применимость, однако он позволяет задействовать меньше вычислительных ресурсов. В данной 
работе будем использовать второй подход.

В качестве базовой модели можно взять логистическую регрессию. Строим вычислительный конвейер из TfidfVectorizer, который реализует логику токенизации 
и вычисления признаков TF-IDF и выбранной модели, обучаем модель
(конвейер) на всей тренировочной выборке, применяем к тестовой и вычисляем значения метрики
правильности.
Также важно определить влияние параметров токенизатора-векторизатора на работу базовой модели. Для чего следует задаться сеткой 
параметров «извлекателя признаков» (например, таких, как число n-грамм, максимальную и 
минимальную частоты слов и т.д.) и без изменения параметров выбранной базовой модели
произвести поиск лучших параметров (на тренировочной выборке). Вычисляем качество модели с 
параметрами, которые соответствуют лучшей модели, найденной с помощью перекрестной 
валидации на тренировочной выборке.

Результаты подбора параметров сведены в единую таблицу, в которой приведены лучшие параметры 
и значения целевой метрики.
